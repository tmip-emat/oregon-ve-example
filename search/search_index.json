{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TMIP-EMAT + VisionEval","text":"<p>Disclaimer</p> <p>The views expressed in this documentation do not necessarily represent the opinions of FHWA, and do not constitute an endorsement, recommendation, or specification by FHWA.</p> <p>This documentation is for the integration of the TMIP-EMAT exploratory modeling and analysis tool with the VisionEval strategic planning model. The audience for this documentation is technically sophisticated users looking for succinct instructions on successfully integrating TMIP-EMAT with VisionEval. Completing such an integration requires at least a rudimentary understanding of both tools, and it is necessary to write or edit Python code to complete the integration.</p> <p>For this integration, we are using VisionEval as a \"core model\", and wrapping it with EMAT to provide a more robust and flexible exploratory modeling and analysis environment.</p> <p>In addition to this documentation, you may also want to refer to the TMIP-EMAT documentation and the VisionEval documentation.</p> <p>This documentation is a companion to two example repositories:</p> <ul> <li>tmip-emat/tmip-emat-ve - An initial     technological demonstration of the integration of EMAT and VisionEval.</li> <li>tmip-emat/ve-integration -     An example of using the integration as implemented for Oregon DOT.</li> </ul>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Exploratory Scoping</li> <li>Running Experiments</li> </ul>"},{"location":"running/","title":"Running Experiments","text":"<p>tl;dr</p> <p>When you run a VisionEval model, it takes a bunch of files as input, it does some stuff, and then it gives you a bunch of files as output. To run an experiment from EMAT, we need to set up the input files to reflect the values of policy levers and exogenous uncertainties for that experiment, run VisionEval model to get the outputs, then extract whatever performance measures we want from those outputs and feed them back to EMAT.</p> <p>The idea behind EMAT is to run a number of experiments, and then analyze the results of those experiments. The number of experiments that needs to be run is a function of the level of complexity of the EMAT scope, but in general it is more experiments that a user would want to run manually. Thus, the EMAT toolset is designed to automate the process of running experiments.</p> <p>When working with VisionEval, at least as defined in this demonstration repository, we will be treating the VisionEval model as a \"files-based core model\". Doing so requires a few steps for each experiment:</p> <ul> <li>Prepare the input files for the VisionEval model, based on the values of policy     levers and exogenous uncertainties defined for the experiment.</li> <li>Run the VisionEval model, using the input files that have been prepared.</li> <li>(Optional) Run any post-processing steps that are needed to extract the results     of the experiment from the output files of the VisionEval model.</li> <li>Collect the output files from the VisionEval model and parse then to extract the     results of the experiment.</li> </ul> <p>Each of these steps is encapsulated in a Python function that is part of the <code>FilesCoreModel</code> interface. In the implementation code, you will see a class that is a subclass of <code>FilesCoreModel</code>, and that class will define the specific steps needed to prepare the input files, run the model, and extract the results.</p> <pre><code>from emat.model.core_files import FilesCoreModel\n\n\nclass VEModel(FilesCoreModel):  # (1)!\n    \"\"\"\n    A class for using Vision Eval as a files core model.\n    \"\"\"\n\n    ...\n</code></pre> <ol> <li>The <code>VEModel</code> class is a subclass of <code>FilesCoreModel</code>, which defines the specific     steps needed to prepare the input files, run the model, and extract the results.</li> </ol> <p>You can see some examples of the <code>FilesCoreModel</code> interface here and here. The process for creating a new analysis with EMAT and VisionEval includes creating a similar class that is a subclass of <code>FilesCoreModel</code>, and then defining the specific methods needed to carry out the steps of the integration. This can be done from scratch, or by copying and modifying an existing example.</p>"},{"location":"running/#setting-up-an-experiment","title":"Setting Up an Experiment","text":"<p>Each experiment involves making a complete copy of the VisionEval model in a contained environment, and then modifying the input files for that copy of the VisionEval model to reflect the specific values of policy levers and exogenous uncertainties for that experiment. The <code>FilesCoreModel</code> interface defines the <code>setup</code> method as the place to create a new copy of the VisionEval model in a contained environment, and then modify the input files for that copy of the VisionEval model. The <code>setup</code> method needs to be overloaded in a subclass of <code>FilesCoreModel</code> to define the specific steps needed to modify the input files for the experiment.</p> <pre><code>class VEModel(FilesCoreModel):\n    ...\n\n    def setup(self, params: dict):  # (1)!\n        \"\"\"\n        Configure the core model with the experiment variable values.\n\n        Args:\n            params (dict):\n                experiment variables including both exogenous\n                uncertainty and policy levers\n\n        Raises:\n            KeyError:\n                if a defined experiment variable is not supported\n                by the core model\n        \"\"\"\n</code></pre> <ol> <li>The <code>setup</code> method accepts a dictionary of parameters, which includes the values of     policy levers and exogenous uncertainties for the experiment.</li> </ol> <p>Within the <code>setup</code> method, the subclass of <code>FilesCoreModel</code> will need to make a complete copy of the VisionEval model in a contained environment, and then modify the input files for that copy of the VisionEval model to reflect the specific values of policy levers and exogenous uncertainties for that experiment.</p> <p>There are numerous possible ways to prepare the input files for the VisionEval model, depending on the exploratory scope and the types of inputs that need to be modified. This demo repository includes a few different examples of how to prepare input files based on the scope:</p> <ul> <li>Categorical Drop-In</li> <li>Mixture of Data Tables</li> <li>Scaling Data Tables</li> <li>Additive Data Tables</li> <li>Template Injection</li> <li>Direct Injection</li> <li>Custom Methods</li> </ul> <p>Each of these methods can be implemented in a bespoke manner for each specific input parameter (both policy levers and exogenous uncertainties), or you can use generic methods that can be applied to a wide range of input parameters. The generic approach is shown in the example repositories.</p>"},{"location":"running/#categorical-drop-in","title":"Categorical Drop-In","text":"<p>Many of the input files for VisionEval are in the form of CSV files. The simplest way to actuate a change in the input files is to simply select an entire file that has the desired values, and copy that file into the requisite input location. This is limited to categorical inputs, which are inputs that can be represented as discrete categorical values. For example, you may have two different population projections, one that represents scenario \"A\" where a particular brownfield area is cleaned up and developed, and another that represents scenario \"B\" where the brownfield is left as is. Under this policy lever, it doesn't make sense to have an intermediate value (\"we'll just clean up part of the toxic waste, and let only few people move in\").</p> <p>An advantage of this method is that it is simple to implement, and it places no limits on the format of the input files. There is no need to have a specific format or a matching number of rows or columns in the input files. In the population projection example considered above, the input files for the two scenarios could have different numbers of rows as one of the two scenarios could imply a different zonal structure within the region.</p> <p>In this example repository, this approach is called the \"categorical drop-in\" method.\\ The VisionEval model will either use the inputs file \"A\" or the inputs file \"B\", but not a mix of the two. This is expressed in the code by the <code>categorical_drop_in</code> method, which is a method of the <code>FilesCoreModel</code>.</p> <pre><code>def _manipulate_by_categorical_drop_in(\n    self,\n    params: dict,  # (1)!\n    cat_param: str,  # (2)!\n    ve_scenario_dir: os.PathLike,  # (3)!\n):\n    scenario_dir = params[cat_param]\n    for i in os.scandir(scenario_input(ve_scenario_dir, scenario_dir)):  # (4)!\n        if i.is_file():\n            shutil.copyfile(\n                scenario_input(ve_scenario_dir, scenario_dir, i.name),\n                join_norm(self.resolved_model_path, \"inputs\", i.name),\n            )\n</code></pre> <ol> <li>The <code>params</code> dictionary is passed through to the <code>_manipulate_by_categorical_drop_in</code>     method. This dictionary includes the values of all the policy levers and exogenous     uncertainties for the experiment.</li> <li>The <code>cat_param</code> argument is the name of the parameter in the <code>params</code> dictionary that     is the categorical drop-in.</li> <li>The <code>ve_scenario_dir</code> argument is the directory where the categorical input files for     the categorical drop-in are stored.</li> <li>The <code>_manipulate_by_categorical_drop_in</code> method will scan the appropriate directory\\     where the categorical input files are stored, and copy the input files for the selected     categorical value into the requisite input location for the VisionEval model.</li> </ol> <p>This method is in turn called from individual <code>setup</code> sub-methods, which will define the specific input parameters that are categorical drop-ins. For example, the <code>_manipulate_carsvcavail</code> method can define the specific input parameters that are categorical drop-ins for car service availability inputs.</p> <pre><code>def _manipulate_carsvcavail(self, params):\n    return self._manipulate_by_categorical_drop_in(\n        params,  # (1)!\n        \"CARSVCAVAILSCEN\",  # (2)!\n        self.scenario_input_dirs.get(\"CARSVCAVAILSCEN\"),  # (3)!\n    )\n</code></pre> <ol> <li>The <code>params</code> dictionary is passed through to the     <code>_manipulate_by_categorical_drop_in</code> method.</li> <li>The second argument to the <code>_manipulate_by_categorical_drop_in</code> method is the     name of the parameter in the <code>params</code> dictionary that is the categorical drop-in,     in this case the <code>CARSVCAVAILSCEN</code> parameter.</li> <li>The third argument to the <code>_manipulate_by_categorical_drop_in</code> method is the     directory where the categorical input files for the categorical drop-in are     stored.</li> </ol> <p>You will find this function mirrored in the EMAT exploratory scope definition, where the categorical drop-in is defined as an uncertainty.</p> <pre><code>inputs:\n  CARSVCAVAILSCEN:\n    shortname: Car Service Availability\n    address: CARSVCAVAILSCEN\n    ptype: exogenous uncertainty\n    dtype: cat # (1)!\n    desc: Different levels of car service availability\n    default: mid # (2)!\n    values: # (3)!\n        - low    \n        - mid    \n        - high   \n</code></pre> <ol> <li>The <code>dtype</code> is set to <code>cat</code> to indicate that this is a categorical input,     which can only take on one of a discrete set of values.</li> <li>The <code>default</code> value is set to <code>mid</code>, which will be the selected value for     this parameter if no other value is specified.</li> <li>The <code>values</code> list defines the discrete set of values that this parameter can     take on. These should be strings, so that we can match against sub-directory     names in the <code>Scenario-Inputs</code> directory of the VisionEval model.</li> </ol> <p>This structure also requires each categorical drop-in to have a corresponding directory in the <code>inputs</code> directory of the VisionEval model, where the input file(s) for each categorical drop-in are stored. Note that there is a directory matching each categorical value, and within that directory are the input files that are to be used when that categorical value is selected. Generally, the names of the input files will be the same across all categorical values, as shown here.</p> <pre><code>\ud83d\udcc1 Scenario-Inputs/\n\u2514\u2500\u2500 \ud83d\udcc1 OTP/\n    \u251c\u2500\u2500 \ud83d\udcc1 ANOTHER_PARAMETER/\n    \u251c\u2500\u2500 \ud83d\udcc1 CARSVCAVAILSCEN/\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 low/\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 marea_carsvc_availability.csv\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 mid/\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 marea_carsvc_availability.csv\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1 high/\n    \u2502       \u2514\u2500\u2500 \ud83d\udcc4 marea_carsvc_availability.csv\n    \u2514\u2500\u2500 \ud83d\udcc1 OTHER_PARAMETER/\n</code></pre>"},{"location":"running/#mixture-of-data-tables","title":"Mixture of Data Tables","text":"<p>In contrast to the categorical drop-in method, the \"mixture of data tables\" method allows for creating \"intermediate\" input files that are a mix of different input files. The approach is suitable for continuous inputs, which are inputs that can take on a range of values. For example, you may have a land use density projection that has upper and lower bounds, and you want to explore the effects of different levels of density between those limits.</p> <p>An advantage of this method is that it allows for a more fine-grained exploration of the input space, and it can be used for continuous inputs. However, it does require that the input files have a specific format (a CSV table containing primarily numeric data), and that the number of rows and columns in the input files match across both the input files, which are labeled as \"1\" and \"2\" in this example.</p> <p>Instead of copying an entire file, the mixture of data tables method will read in both input files, and then linearly interpolate between the two input files based on the value of the policy lever or exogenous uncertainty. This is expressed in the code by the <code>_manipulate_by_mixture</code> method, which is a method of the <code>FilesCoreModel</code>.</p> <pre><code>def _manipulate_by_mixture(\n    self,\n    params,  # (1)!\n    weight_param,  # (2)!\n    ve_scenario_dir,  # (3)!\n    no_mix_cols=(\n        \"Year\",\n        \"Geo\",\n    ),  # (4)!\n    float_dtypes=False,  # (5)!\n):\n    weight_2 = params[weight_param]\n    weight_1 = 1.0 - weight_2\n\n    # Gather list of all files in directory \"1\", and confirm they\n    # are also in directory \"2\"\n    filenames = []\n    for i in os.scandir(scenario_input(ve_scenario_dir, \"1\")):\n        if i.is_file():\n            filenames.append(i.name)\n            f2 = scenario_input(ve_scenario_dir, \"2\", i.name)\n            if not os.path.exists(f2):\n                raise FileNotFoundError(f2)\n\n    for filename in filenames:\n        df1 = pd.read_csv(scenario_input(ve_scenario_dir, \"1\", filename))\n        isna_ = (df1.isnull().values).any()\n        df1.fillna(0, inplace=True)  # (6)!\n        df2 = pd.read_csv(scenario_input(ve_scenario_dir, \"2\", filename))\n        df2.fillna(0, inplace=True)\n\n        float_mix_cols = list(df1.select_dtypes(\"float\").columns)\n        if float_dtypes:\n            float_mix_cols = float_mix_cols + list(df1.select_dtypes(\"int\").columns)\n        for j in no_mix_cols:\n            if j in float_mix_cols:\n                float_mix_cols.remove(j)\n\n        if float_mix_cols:\n            df1_float = df1[float_mix_cols]\n            df2_float = df2[float_mix_cols]\n            df1[float_mix_cols] = df1_float * weight_1 + df2_float * weight_2\n\n        int_mix_cols = list(df1.select_dtypes(\"int\").columns)\n        if float_dtypes:\n            int_mix_cols = list()\n        for j in no_mix_cols:\n            if j in int_mix_cols:\n                int_mix_cols.remove(j)\n\n        if int_mix_cols:\n            df1_int = df1[int_mix_cols]\n            df2_int = df2[int_mix_cols]\n            df_int_mix = df1_int * weight_1 + df2_int * weight_2\n            df1[int_mix_cols] = np.round(df_int_mix).astype(int)  # (7)!\n\n        out_filename = join_norm(self.resolved_model_path, \"inputs\", filename)\n        if isna_:\n            df1.replace(0, np.nan, inplace=True)\n        df1.to_csv(out_filename, index=False, float_format=\"%.5f\", na_rep=\"NA\")\n</code></pre> <ol> <li>The <code>params</code> dictionary is passed through to the <code>_manipulate_by_mixture</code> method.</li> <li>The <code>weight_param</code> argument is the name of the parameter in the <code>params</code> dictionary     that is the weight for the mixture of data tables.</li> <li>The <code>ve_scenario_dir</code> argument is the directory where the input files for the     mixture of data tables are stored. There should be two subdirectories, \"1\" and \"2\".</li> <li>The <code>no_mix_cols</code> argument is a list of column names that should not be mixed. This     is useful for columns that are not numerical, such as year or geography, which should     not be mixed (or for which there is no reasonable linear interpolation). These columns     will be copied from the input file in directory \"1\" to the output file.</li> <li>The <code>float_dtypes</code> argument is a boolean that indicates whether integer columns     should be treated as float columns for the purposes of mixing. Setting this     to <code>True</code> will treat integer columns as float columns, and will mix them as such,     which can be problematic if VisionEval is expecting integers.</li> <li>The <code>isna_</code> variable is set to <code>True</code> if there are any <code>NaN</code> values in the input     file. If there are, these will be replaced with zeros for the purposes of mixing,     and then replaced with <code>NaN</code> in the output file, as linear interpolation of <code>NaN</code>     values is not possible.</li> <li>The <code>df_int_mix</code> variable is the linear interpolation of the integer columns, and     is optionally rounded to the nearest integer. This is done to ensure that the output file     has integer values, which is important if VisionEval is expecting integers.</li> </ol> <p>This method is in turn called from individual <code>setup</code> sub-methods, which will define the specific input parameters that are mixtures of data tables. For example, the <code>_manipulate_landuse</code> method can define the specific input parameters that are mixtures of data tables for land use density inputs.</p> <pre><code>def _manipulate_ludensity(self, params):\n    return self._manipulate_by_mixture(\n        params,  # (1)!\n        \"LUDENSITYMIX\",  # (2)!\n        self.scenario_input_dirs.get(\"LUDENSITYMIX\"),  # (3)!\n    )\n</code></pre> <ol> <li>The <code>params</code> dictionary is passed through to the     <code>__manipulate_by_mixture</code> method.</li> <li>The second argument to the <code>_manipulate_by_mixture</code> method is the     name of the parameter in the <code>params</code> dictionary that is controlling the,     mixture, in this case the <code>LUDENSITYMIX</code> parameter.</li> <li>The third argument to the <code>__manipulate_by_mixture</code> method is the     directory where the categorical input files for the mixture bounds are     stored.</li> </ol> <p>You will find this function mirrored in the EMAT exploratory scope definition, where the mixture of data tables is defined as an exogenous uncertainty.</p> <pre><code>inputs:\n    LUDENSITYMIX:\n        shortname: Urban Mix Prop\n        address: LUDENSITYMIX\n        ptype: exogenous uncertainty\n        dtype: float # (1)!\n        desc: Urban proportion for each marea by year\n        default: 0\n        min: 0 # (2)!\n        max: 1 # (3)!\n</code></pre> <ol> <li>The <code>dtype</code> is set to <code>float</code> to indicate that this is a continuous input,     which can take on a range of values.</li> <li>The <code>min</code> value for mixtures is always set to <code>0</code>, which represents the lower bound for this     parameter, and will set the weight of the \"1\" input file to <code>1.0</code> and the     weight of the \"2\" input file to <code>0.0</code>.</li> <li>The <code>max</code> value for mixtures is always set to <code>1</code>, which represents the upper bound for this     parameter, and will set the weight of the \"1\" input file to <code>0.0</code> and the     weight of the \"2\" input file to <code>1.0</code>.</li> </ol> <p>This structure also requires each mixture to have a corresponding directory in the <code>inputs</code> directory of the VisionEval model, where the input file(s) for each categorical drop-in are stored. Note that there are exactly two sub-directories in this parameters directory, and they are named \"1\" and \"2\", and within those two directories are the input files that are to be mixed together. The names of the input file(s) must be the same across all both sub-directories, as shown here, and they must be in the same format (a CSV table containing primarily numeric data).</p> <pre><code>\ud83d\udcc1 Scenario-Inputs/\n\u2514\u2500\u2500 \ud83d\udcc1 OTP/\n    \u251c\u2500\u2500 \ud83d\udcc1 ANOTHER_PARAMETER/\n    \u251c\u2500\u2500 \ud83d\udcc1 LUDENSITYMIX/\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 1/\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 marea_mix_targets.csv\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1 2/\n    \u2502       \u2514\u2500\u2500 \ud83d\udcc4 marea_mix_targets.csv\n    \u2514\u2500\u2500 \ud83d\udcc1 OTHER_PARAMETER/\n</code></pre>"},{"location":"running/#scaling-data-tables","title":"Scaling Data Tables","text":"<p>The scaling data tables method is much like the mixture of data tables method, but instead of linearly interpolating between two input files, the scaling data tables method will scale all the values in selected columns of an input file up or down based on the value of the policy lever or exogenous uncertainty. This is useful for continuous inputs that are best represented as a single table, but where the values in that table can be scaled up or down. For example, you may have a population projection that represents a \"baseline\" scenario, and you want to explore the effects of different levels of population growth.</p> <p>The <code>_manipulate_by_scale</code> function shown below can be included in an integration's subclass of <code>FilesCoreModel</code>, and used to scale the values in the input files based on the value of the policy lever or exogenous uncertainty.</p> <pre><code>def _manipulate_by_scale(\n    self,\n    params,  # (1)!\n    param_map,  # (2)!\n    ve_scenario_dir,  # (3)!\n    max_thresh=1e9,  # (4)!\n):\n    # Gather list of all files in scenario input directory\n    filenames = []\n    for i in os.scandir(scenario_input(ve_scenario_dir)):\n        if i.is_file():\n            filenames.append(i.name)\n\n    for filename in filenames:\n        df1 = pd.read_csv(scenario_input(ve_scenario_dir, filename))\n        for param_name, column_names in param_map.items():\n            if isinstance(column_names, str):\n                column_names = [column_names]\n            for column_name in column_names:\n                df1[[column_name]] = (df1[[column_name]] * params.get(param_name)).clip(\n                    lower=-max_thresh, upper=max_thresh\n                )  # (5)!\n\n        out_filename = join_norm(self.resolved_model_path, \"inputs\", filename)\n        df1.to_csv(out_filename, index=False, float_format=\"%.5f\", na_rep=\"NA\")\n</code></pre> <ol> <li>The <code>params</code> dictionary is passed through from the <code>setup</code> method to the     <code>_manipulate_by_scale</code> method.</li> <li>The <code>param_map</code> argument is a dictionary that maps the parameter names in the     <code>params</code> dictionary to the column names in the input file that should be scaled.</li> <li>The <code>ve_scenario_dir</code> argument is the directory where the input files for the     scaling are stored.</li> <li>The <code>max_thresh</code> argument is the maximum value that any value in the input file     can be scaled to. This is important to ensure that the scaled values are not too     large or too small.</li> <li>The <code>clip</code> method is used to ensure that the scaled values are not too large or too     small. This is important to ensure that the scaled values are within the range of     values that VisionEval is expecting.</li> </ol> <p>If you use this approach, you would not set the <code>min</code> and <code>max</code> values for the relevant parameter in the exploratory scope definition to 0 and 1, as you would for the mixture model. Instead, set those limits to the minimum and maximum values that you want to use for the scaling factor. The upper and lower limits need not be symmetric around 1.0, as the scaling factor can be used to scale values up or down, or both.</p> <pre><code>inputs:\n    LUDENSITYMIX:\n        shortname: Urban Mix Prop\n        address: LUDENSITYMIX\n        ptype: exogenous uncertainty\n        dtype: float # (1)!\n        desc: Urban proportion for each marea by year\n        default: 0\n        min: 0.75 # (2)!\n        max: 1.5 # (3)!\n</code></pre> <ol> <li>The <code>dtype</code> is set to <code>float</code> to indicate that this is a continuous input,     which can take on a range of values.</li> <li>The <code>min</code> value for scaling factor can be any value. Positive values less     than or equal to 1.0 are most common, but negative values are also allowable,     if the signs on the targeted values might be inverted.</li> <li>The <code>max</code> value for the scaling factor can be any value. Positive values     greater than or equal to 1.0 are most common.</li> </ol> <p>The function <code>__manipulate_by_scale</code> written above also implies that the input files on which the scaling factor are applied are in the scenario directory, not a subdirectory of the scenario directory, as was the case in mixture models.\\ This is because the scaling factor method is applied to a single set of inputs, so there's no need to have multiple subdirectories for the input files.</p> <pre><code>\ud83d\udcc1 Scenario-Inputs/\n\u2514\u2500\u2500 \ud83d\udcc1 OTP/\n    \u251c\u2500\u2500 \ud83d\udcc1 ANOTHER_PARAMETER/\n    \u251c\u2500\u2500 \ud83d\udcc1 LUDENSITYMIX/\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 marea_mix_targets.csv\n    \u2514\u2500\u2500 \ud83d\udcc1 OTHER_PARAMETER/\n</code></pre>"},{"location":"running/#additive-data-tables","title":"Additive Data Tables","text":"<p>Forthcoming: documentation of the <code>_manipulate_by_delta</code> method, which allows mixtures based on additive deltas instead of linear interpolation.</p>"},{"location":"running/#template-injection","title":"Template Injection","text":"<p>Forthcoming: documentation of the template injection method, which writes parameter values directly into the input files based on a template.</p>"},{"location":"running/#direct-injection","title":"Direct Injection","text":"<p>Forthcoming: documentation of the direct injection method, which writes parameter values directly into the input files, overwriting existing values.</p>"},{"location":"running/#custom-methods","title":"Custom Methods","text":"<p>Forthcoming: documentation of how to define custom methods for preparing input files.</p>"},{"location":"running/#running-an-experiment","title":"Running an Experiment","text":"<p>Once the input files have been prepared, the VisionEval model can be run. The <code>FilesCoreModel</code> interface defines the <code>run</code> method as the place to run the VisionEval model. The <code>run</code> method needs to be overloaded in a subclass of <code>FilesCoreModel</code> to define the specific steps needed to run the VisionEval model.</p> <p>In this example, the main thing we do in the <code>run</code> method is to set the <code>path</code> environment variable to include the path to the R executable, and then run s small script that opens the VisionEval model and runs it with the desired inputs.</p> <pre><code>class VEModel(FilesCoreModel):\n    ...\n\n    def run(self):  # (1)!\n        os.environ[\"path\"] = (\n            join_norm(self.config[\"r_executable\"]) + \";\" + os.environ[\"path\"]\n        )\n        cmd = \"Rscript\"\n\n        # write a small script that opens the model and runs it\n        with open(join_norm(self.local_directory, \"vemodel_runner.R\"), \"wt\") as script:\n            script.write(f\"\"\"\n            thismodel &lt;- openModel(\"{r_join_norm(self.local_directory, self.modelname)}\")\n            thismodel$run(\"reset\")\n            \"\"\")\n\n        self.last_run_result = subprocess.run(  # (2)!\n            [cmd, \"vemodel_runner.R\"],\n            cwd=self.local_directory,\n            capture_output=True,\n        )\n</code></pre> <ol> <li>The <code>run</code> method accepts no arguments. All the information needed to run the     experiment is stored in files written during the <code>setup</code> method.</li> <li>The subprocess.run command runs a command line tool. The     name of the command line tool, plus all the command line arguments     for the tool, are given as a list of strings, not one string.     The <code>cwd</code> argument sets the current working directory from which the     command line tool is launched. Setting <code>capture_output</code> to True     will capture both stdout and stderr from the command line tool, and     make these available in the result object to facilitate debugging.</li> </ol>"},{"location":"running/#extracting-results","title":"Extracting Results","text":"<p>Once the VisionEval model has been run, the output files need to be collected and parsed to extract the performance measures that quantify the results of the experiment. The <code>FilesCoreModel</code> interface defines some standardized data extraction processes that can be configured entirely in the exploratory scope (i.e. the YAML config file), so it may not be necessary to write Python code to extract the results.</p> <p>Each performance measure that is to be extracted from the output files of the VisionEval model is defined in the exploratory scope definition, under the <code>outputs</code> section. Each output is defined by a unique name, and the <code>parser</code> section of the output definition defines how to extract the performance measure from the output files. The <code>parser</code> section can include a <code>file</code> argument, which is the name of the output file from which the performance measure is to be extracted. It can also include a <code>loc</code> or an <code>iloc</code> argument, which is the location in the output file where the performance measure is to be found. These locations correlate with the <code>pandas.DataFrame</code> accessors <code>loc</code> and <code>iloc</code>, respectively. For the <code>loc</code> argument, file is read in as a <code>pandas.DataFrame</code>, with the first row as the column names, and the first column as the index, and the performance measure is extracted by selecting the row &amp; column with the labels that match the value of the <code>loc</code> argument. For the <code>iloc</code> argument, the performance measure is extracted by selecting the row &amp; column with the integer positions that match the value of the <code>iloc</code> argument.</p> <p>Since most</p> <pre><code>outputs:\n    HouseholdDvmtPerHh:\n        kind: info\n        desc: Average daily vehicle miles traveled by households in 2050\n        metamodeltype: linear\n        parser:\n            file: state_validation_measures.csv\n            loc: [HouseholdDvmtPerHh, 2050]\n</code></pre> <p>Since most VisionEval output files are in CSV format, this simple parsing will be sufficient for most cases. However, if the desired performance measures is not directly available as a scalar value in an output files, there is also an <code>eval</code> parser that can be used to evaluate a Python expression that computes the desired result. For example, this parser will compute the difference between the 2010 and 2038 values of the <code>UrbanHhCO2e</code> values, and report the difference as the performance measure.</p> <pre><code>outputs:\n    UrbanHhCO2eReduction:\n        shortname: Cars GHG Reduction\n        kind: info\n        desc: Reduction from 2010 level in average annual production of greenhouse gas emissions from light-duty\n            vehicle travel by households residing in the urban area\n        transform: none\n        metamodeltype: linear\n        parser:\n            file: Measures_VERSPM_2010,2038_Marea=RVMPO.csv\n            eval: loc['UrbanHhCO2e','2010'] - loc['UrbanHhCO2e','2038']\n</code></pre> <p>For more complex parsing and analysis, you can define a custom Python function to compute arbitrarily complex performance measures. In the example repository, the Oregon VE State model has a custom parser written in R that computes a number of performance measures. This R script is called as a subprocess from the <code>post_process</code> method of the <code>FilesCoreModel</code> subclass. The built-in parser described above is then used to extract the performance measures from the output of the R post-processing script.</p>"},{"location":"scoping/","title":"Exploratory Scoping","text":"<p>This section is forthcoming.</p>"}]}